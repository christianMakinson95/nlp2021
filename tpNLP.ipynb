{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5f5aa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dataclasses import make_dataclass\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import pandas_datareader as web\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import seaborn as sns\n",
    "import bitfinex as btf\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "compT, compR, compG = [], [],[]\n",
    "negT, negR, negG = [], [],[]\n",
    "posT, posR, posG = [], [],[]\n",
    "neuT, neuR, neuG = [], [],[]\n",
    "SIAT,SIAR, SIAG = 0, 0, 0\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_datareader as web\n",
    "import tweepy\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from datetime import timedelta\n",
    "from dataclasses import make_dataclass\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import seaborn as sns\n",
    "import bitfinex as btf\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "\n",
    "import praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e94ebfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear una funcion para guardar puntajes de sentimiento con vaderSentiment\n",
    "def getSIA(text):\n",
    "  sia = SentimentIntensityAnalyzer()\n",
    "  sentiment = sia.polarity_scores(text)\n",
    "  return sentiment\n",
    "def setSIA(field,pos,neg,neu,comp):\n",
    "    for i in range(0, len(field)):\n",
    "      SIA = getSIA(field[i])\n",
    "      comp.append(SIA['compound'])\n",
    "      neg.append(SIA['neg'])\n",
    "      neu.append(SIA['neu'])\n",
    "      pos.append(SIA['pos'])\n",
    "    return [pos,neg,neu,comp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6319363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeleador(df):\n",
    "    labels=[]\n",
    "    for i in range(len(df)):\n",
    "        if df['Open'][i]:\n",
    "            if df['Open'][i]!=\"\" and df['Close'][i]!=\"\":\n",
    "                if int(float(df['Open'][i]))<int(float(df['Close'][i])):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "    df['Label']=labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d87fe8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaVacios(df):\n",
    "    for i in range(len(df)):\n",
    "        if \"nameUser\" in df['nameUser'][i]:\n",
    "            df = df.drop(index=i)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "def limpiaErroresR(df):\n",
    "    for i in range(len(df)):\n",
    "        if \"Title\" in df['Title'][i]:\n",
    "            df = df.drop(index=i)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "def limpiaErroresG(df):\n",
    "    for i in range(len(df)):\n",
    "        if \"Title\" in df['Title'][i]:\n",
    "            df = df.drop(index=i)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e53c5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = pd.read_csv('exportTweetData.csv')\n",
    "df_market = pd.read_csv('exportMarketData.csv')\n",
    "df_reddit = pd.read_csv('exportRedditData.csv')\n",
    "df_google = pd.read_csv('exportNewsData.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "860ebfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetCombinadoTwitter = df_tweet.merge(df_market, how='inner', on='Date')\n",
    "dataSetCombinadoReddit = df_reddit.merge(df_market,how='inner', on='Date')\n",
    "dataSetCombinadoGoogle = df_google.merge(df_market, how='inner',on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bac81677",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetCombinadoTwitter = labeleador(dataSetCombinadoTwitter)\n",
    "\n",
    "dataSetCombinadoReddit = limpiaVacios(dataSetCombinadoReddit)\n",
    "dataSetCombinadoReddit = labeleador(dataSetCombinadoReddit)\n",
    "dataSetCombinadoReddit = limpiaErroresR(dataSetCombinadoReddit)\n",
    "\n",
    "dataSetCombinadoGoogle = limpiaErroresG(dataSetCombinadoGoogle)\n",
    "#dataSetCombinadoGoogle\n",
    "dataSetCombinadoGoogle = labeleador(dataSetCombinadoGoogle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ccf70a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadosSIAT = setSIA(dataSetCombinadoTwitter['Tweets'],posT,negT,neuT,compT)\n",
    "resultadosSIAR = setSIA(dataSetCombinadoReddit['Title'],posR,negR,neuR,compR)\n",
    "resultadosSIAG = setSIA(dataSetCombinadoGoogle['Desc'],posG,negG,neuG,compG)\n",
    "\n",
    "compT = resultadosSIAT[0]\n",
    "negT = resultadosSIAT[1]\n",
    "neuT = resultadosSIAT[2]\n",
    "posT = resultadosSIAT[3]\n",
    "\n",
    "compR = resultadosSIAR[0]\n",
    "negR = resultadosSIAR[1]\n",
    "neuR = resultadosSIAR[2]\n",
    "posR = resultadosSIAR[3]\n",
    "\n",
    "compG = resultadosSIAG[0]\n",
    "negG = resultadosSIAG[1]\n",
    "neuG = resultadosSIAG[2]\n",
    "posG = resultadosSIAG[3]\n",
    "\n",
    "dataSetCombinadoTwitter['Compound'] = compT\n",
    "dataSetCombinadoTwitter['Negative'] = negT\n",
    "dataSetCombinadoTwitter['Neutral'] = neuT\n",
    "dataSetCombinadoTwitter['Positive'] = posT\n",
    "\n",
    "dataSetCombinadoReddit['Compound'] = compR\n",
    "dataSetCombinadoReddit['Negative'] = negR\n",
    "dataSetCombinadoReddit['Neutral'] = neuR\n",
    "dataSetCombinadoReddit['Positive'] = posR\n",
    "\n",
    "dataSetCombinadoGoogle['Compound'] = compG\n",
    "dataSetCombinadoGoogle['Negative'] = negG\n",
    "dataSetCombinadoGoogle['Neutral'] = neuG\n",
    "dataSetCombinadoGoogle['Positive'] = posG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e410ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['Open','High','Low','Volume','Compound','Negative','Neutral','Positive','Label']\n",
    "trainDFLinealTwitter = dataSetCombinadoTwitter[keep_columns]\n",
    "trainDFLinealReddit = dataSetCombinadoReddit[keep_columns]\n",
    "trainDFLinealGoogle = dataSetCombinadoGoogle[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4fbc3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenamiento de programa con modelo lineal\n",
    "#Para modelo Lineal necesitamos np.array\n",
    "X_twitter = trainDFLinealTwitter\n",
    "X_twitter = np.array(X_twitter.drop('Label',1))\n",
    "\n",
    "#Create the target data set\n",
    "Y_twitter = np.array(dataSetCombinadoTwitter['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "71c26ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenamiento de programa con modelo lineal\n",
    "#Para modelo Lineal necesitamos np.array\n",
    "X_reddit = trainDFLinealReddit\n",
    "X_reddit = np.array(X_reddit.drop('Label',1))\n",
    "\n",
    "#Create the target data set\n",
    "Y_reddit = np.array(dataSetCombinadoReddit['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7427a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenamiento de programa con modelo lineal\n",
    "#Para modelo Lineal necesitamos np.array\n",
    "X_google = trainDFLinealGoogle\n",
    "X_google = np.array(X_google.drop('Label',1))\n",
    "\n",
    "#Create the target data set\n",
    "Y_google = np.array(dataSetCombinadoGoogle['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "25ab4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set de entrenamiento Twitter\n",
    "x_train_t, x_test_t, y_train_t, y_test_t = train_test_split(X_twitter,Y_twitter, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "61937789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83        41\n",
      "           1       0.89      0.86      0.88        59\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.85      0.86      0.86       100\n",
      "weighted avg       0.86      0.86      0.86       100\n",
      "\n",
      "0.86\n"
     ]
    }
   ],
   "source": [
    "modelLDT = LinearDiscriminantAnalysis().fit(x_train_t,y_train_t)\n",
    "predLinearT = modelLDT.predict(x_test_t)\n",
    "print(classification_report(y_test_t, predLinearT))\n",
    "print(accuracy_score(y_test_t,predLinearT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "244d3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set de entrenamiento Reddit\n",
    "#x_train_r, x_test_r, y_train_r, y_test_r = train_test_split(X_reddit,Y_reddit, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "33be03f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LinearDiscriminantAnalysis().fit(x_train_r,y_train_r)\n",
    "#predLinearR = model.predict(x_test_r)\n",
    "#print(classification_report(y_test_r, predLinearR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a3044a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set de entrenamiento Google\n",
    "x_train_g, x_test_g, y_train_g, y_test_g = train_test_split(X_google,Y_google, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c0e9bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.90      0.75      0.78         6\n",
      "weighted avg       0.87      0.83      0.81         6\n",
      "\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "model = LinearDiscriminantAnalysis().fit(x_train_g,y_train_g)\n",
    "predLinearG = model.predict(x_test_g)\n",
    "print(classification_report(y_test_g, predLinearG))\n",
    "print(accuracy_score(y_test_g,predLinearG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e2af49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagg of word para twitter\n",
    "bow_vectorizerT = CountVectorizer(max_df=0.9, min_df=2, max_features=1000, stop_words='english')\n",
    "bowT = bow_vectorizerT.fit_transform(dataSetCombinadoTwitter['Tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2a84c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagg of word para google\n",
    "bow_vectorizerG = CountVectorizer(max_df=0.9, min_df=2, max_features=1000, stop_words='english')\n",
    "bowG = bow_vectorizerG.fit_transform(dataSetCombinadoGoogle['Desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "71ebaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagg of word para reddit\n",
    "#bow_vectorizerR = CountVectorizer(max_df=0.9, min_df=2, max_features=1000, stop_words='english')\n",
    "#bowR = bow_vectorizerR.fit_transform(dataSetCombinadoReddit['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8923a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenamiento de programa con modelo regresivo Twitter\n",
    "x_train_t, x_test_t, y_train_t, y_test_t = train_test_split(bowT,dataSetCombinadoTwitter['Label'], random_state = 42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f32e5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenamiento de programa con modelo regresivo Google\n",
    "x_train_g, x_test_g, y_train_g, y_test_g = train_test_split(bowG,dataSetCombinadoGoogle['Label'], random_state = 42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8ae68812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenamiento de programa con modelo regresivo Reddit\n",
    "#x_train, x_test, y_train, y_test = train_test_split(bowR,dataSetCombinado['Label'], random_state = 42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "65b39dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.66      0.65        53\n",
      "           1       0.75      0.74      0.74        72\n",
      "\n",
      "    accuracy                           0.70       125\n",
      "   macro avg       0.70      0.70      0.70       125\n",
      "weighted avg       0.70      0.70      0.70       125\n",
      "\n",
      "0.704\n"
     ]
    }
   ],
   "source": [
    "modelLRT = LogisticRegression().fit(x_train_t,y_train_t)\n",
    "predRegT = modelLRT.predict(x_test_t)\n",
    "print(classification_report(y_test_t, predRegT))\n",
    "print(accuracy_score(y_test_t,predRegT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f571f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.86         7\n",
      "   macro avg       0.43      0.50      0.46         7\n",
      "weighted avg       0.73      0.86      0.79         7\n",
      "\n",
      "0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "modelLRG = LogisticRegression().fit(x_train_g,y_train_g)\n",
    "predRegG = modelLRG.predict(x_test_g)\n",
    "print(classification_report(y_test_g, predRegG))\n",
    "print(accuracy_score(y_test_g,predRegG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cb0a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#newPred = model.predict(df_tweet['Tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12434682",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = dataSetCombinadoTwitter['Tweets'].tolist()\n",
    "lblT = dataSetCombinadoTwitter['Label'].tolist()\n",
    "\n",
    "dataR = dataSetCombinadoReddit['Title'].tolist()\n",
    "lblR = dataSetCombinadoReddit['Label'].tolist()\n",
    "\n",
    "dataG = dataSetCombinadoGoogle['Desc'].tolist()\n",
    "lblG = dataSetCombinadoGoogle['Label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2dcaa39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "vectorizerT = TfidfVectorizer(max_features = 2500, max_df=0.8, stop_words = stopwords.words('english'))\n",
    "dataT = vectorizerT.fit_transform(dataT).toarray()\n",
    "\n",
    "#vectorizerR = TfidfVectorizer(max_features = 2500, max_df=0.8, stop_words = stopwords.words('english'))\n",
    "#dataR = vectorizerR.fit_transform(dataR).toarray()\n",
    "\n",
    "vectorizerG = TfidfVectorizer(max_features = 2500, max_df=0.8, stop_words = stopwords.words('english'))\n",
    "dataG = vectorizerG.fit_transform(dataG).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "31d8d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_t, x_test_t, y_train_t, y_test_t = train_test_split(dataT, lblT, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#x_train_r, x_test_r, y_train_r, y_test_r = train_test_split(dataR, lblR, test_size = 0.2, random_state = 0)\n",
    "\n",
    "x_train_g, x_test_g, y_train_g, y_test_g = train_test_split(dataG, lblG, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9831962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifierT = RandomForestClassifier(n_estimators = 200, random_state = 0)\n",
    "text_classifierT.fit(x_train_t, y_train_t)\n",
    "\n",
    "#text_classifierR = RandomForestClassifier(n_estimators = 200, random_state = 0)\n",
    "#text_classifierR.fit(x_train_r, y_train_r)\n",
    "\n",
    "text_classifierG = RandomForestClassifier(n_estimators = 200, random_state = 0)\n",
    "text_classifierG.fit(x_train_g, y_train_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1ff85812",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsT = text_classifierT.predict(x_test_t)\n",
    "\n",
    "#predictionsR = text_classifierR.predict(x_test_r)\n",
    "\n",
    "predictionsG = text_classifierG.predict(x_test_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b395a247",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [125, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-76f953a5e684>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictionsT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictionsT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictionsT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m     \"\"\"\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [125, 100]"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_t,predictionsT))\n",
    "print(classification_report(y_test_t,predictionsT))\n",
    "print(accuracy_score(y_test_t,predictionsT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c98abd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_r,predictionsR))\n",
    "#print(classification_report(y_test_r,predictionsR))\n",
    "#print(accuracy_score(y_test_r,predictionsR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "234de56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [0 4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.33      0.50      0.40         6\n",
      "weighted avg       0.44      0.67      0.53         6\n",
      "\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_g,predictionsG))\n",
    "print(classification_report(y_test_g,predictionsG))\n",
    "print(accuracy_score(y_test_g,predictionsG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bd7972f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gauseano\n",
    "modelT = GaussianNB()\n",
    "modelG = GaussianNB()\n",
    "modelR = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b4f49236",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-62e3005f5576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodelT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodelG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_g\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    651\u001b[0m                                       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m                                       \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[0;32m    418\u001b[0m                         \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "modelT.fit(x_train_t, y_train_t)\n",
    "modelG.fit(x_train_g, y_train_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9b741d4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This GaussianNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-0c459b50dc8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictionT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpredictionG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_g\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This GaussianNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "predictionT = modelT.predict(x_test_t)\n",
    "print(x_test_t)\n",
    "predictionG = modelG.predict(x_test_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82954e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24 17]\n",
      " [37 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.59      0.47        41\n",
      "           1       0.56      0.37      0.45        59\n",
      "\n",
      "    accuracy                           0.46       100\n",
      "   macro avg       0.48      0.48      0.46       100\n",
      "weighted avg       0.49      0.46      0.46       100\n",
      "\n",
      "0.46\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_t,predictionsT))\n",
    "print(classification_report(y_test_t,predictionsT))\n",
    "print(accuracy_score(y_test_t,predictionsT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be236fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [0 4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.33      0.50      0.40         6\n",
      "weighted avg       0.44      0.67      0.53         6\n",
      "\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_g,predictionsG))\n",
    "print(classification_report(y_test_g,predictionsG))\n",
    "print(accuracy_score(y_test_g,predictionsG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c75827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mejor Score\n",
    "#Twitter -> Regression ~= 70\n",
    "#Google -> Regression ~= 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "71f7aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acceso a Twitter API\n",
    "consumerKey = \"TwlSxQ7X0qOJ9XPYi0RRGFmPy\"\n",
    "consumerSecret = \"4QCBGcgWOOhHWXe0SwDu5UMVGVSZvnZujkHhacliUQeBigM7ZS\"\n",
    "accessToken = \"1083093878893432836-HW9vv7t1xihZvZvq0YdomkNKPzFnWe\"\n",
    "accessTokenSecret = \"PW8gxDGBCaNcprbkIb0uEQstg5tUzL55KRWEOI8Vqz2U9\"\n",
    "\n",
    "authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "authenticate.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(authenticate, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2c6eae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LimpiaTexto\n",
    "def cleanText(twt):\n",
    "    EMOJI_PATTERN = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\" \n",
    "    \"]+\"\n",
    "    )\n",
    "    twt = re.sub('#[A-Za-z0-9]+','',twt) #Removes any string with hash\n",
    "    twt = re.sub(r'[0-9]+', '', twt)\n",
    "    twt = re.sub('@[A-Za-z0-9]+','',twt) #Removes any string with at\n",
    "    twt = re.sub('\\\\n','',twt) #Remove \\n caracter\n",
    "    twt = EMOJI_PATTERN.sub(r'', twt)\n",
    "    twt = twt.replace('!','').replace('-','').replace('_','')\n",
    "    twt = re.sub('https?:\\/\\/\\S+','',twt) #Removes hastag\n",
    "    return twt\n",
    "def cleanTime(time):\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M\")\n",
    "def cleanTimeStamp(time):\n",
    "    return datetime.fromtimestamp(time).strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "edb03ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No keys, only access to public API functions\n"
     ]
    }
   ],
   "source": [
    "analyzedCrypto = 'BTC'\n",
    "#CREAR DATASET DE TWEETS\n",
    "cantidadDeTweetsBuscados = 500\n",
    "#loopeo para obtener y almacenar info de todas las monedas\n",
    "\n",
    "search_term = analyzedCrypto+' -filter:retweets'\n",
    "#mySince = datetime.now()-timedelta(minutes=30)\n",
    "#mySince = mySince.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#myUntil = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "mySince=\"2021-07-2\"\n",
    "myUntil=\"2021-07-2\"\n",
    "tweets = tweepy.Cursor(api.search, q=search_term, lang ='en', since=mySince, until=myUntil, tweet_mode='extended').items(cantidadDeTweetsBuscados)\n",
    "all_tweets = [tweet.full_text for tweet in tweets]\n",
    "\n",
    "tweets = tweepy.Cursor(api.search, q=search_term, lang ='en', since=mySince, until=myUntil,  tweet_mode='extended').items(cantidadDeTweetsBuscados)\n",
    "all_dates = [tweet.created_at for tweet in tweets]\n",
    "\n",
    "\n",
    "formatoDataSetTweets = {'Date': all_dates, 'Tweets': all_tweets}\n",
    "df_tweets = pd.DataFrame(data=formatoDataSetTweets)\n",
    "\n",
    "df_tweets['Tweets'] = df_tweets['Tweets'].apply(cleanText)\n",
    "df_tweets['Date'] = df_tweets['Date'].apply(cleanTime)\n",
    "\n",
    "#Genero dataSet con valores financieros\n",
    "api_v2 = btf.bitfinex_v2.api_v2()\n",
    "\n",
    "#CREAR DATASET DE VALORES DE MERCADO\n",
    "pair = analyzedCrypto\n",
    "TIMEFRAME = '1m'\n",
    "\n",
    "t_start = datetime(2020,7,2,0,0)\n",
    "t_start = time.mktime(t_start.timetuple())*1000\n",
    "\n",
    "t_stop = datetime(2021,7,2,0,0)\n",
    "t_stop = time.mktime(t_stop.timetuple())*1000\n",
    "\n",
    "result = api_v2.candles(symbol = 'BTCUSD', interval = TIMEFRAME, limit = 5000, start = t_start, end = t_stop)\n",
    "\n",
    "camposDataFrame = ['Date','Open', 'Close', 'High', 'Low', 'Volume']\n",
    "df_market = pd.DataFrame(result, columns=camposDataFrame)\n",
    "df_market['Date'] = pd.to_datetime(df_market['Date'], unit='ms')\n",
    "df_market['Date'] = df_market['Date'].apply(cleanTime)\n",
    "\n",
    "\n",
    "googlenews=GoogleNews(start='07/2/2021',end='07/2/2021')\n",
    "googlenews.search('Bitcoin')\n",
    "result=googlenews.result()\n",
    "camposDataFrame = ['title','media', 'datetime', 'desc']\n",
    "df_news=pd.DataFrame(result,columns=camposDataFrame)\n",
    "df_news['title']=df_news['title'].apply(cleanText)\n",
    "df_news['desc']=df_news['desc'].apply(cleanText)\n",
    "df_news.columns = ['Title','Media', 'Date', 'Desc']\n",
    "df_news['Date'] = df_news['Date'].apply(cleanTime)\n",
    "\n",
    "dataSetCombinadoGoogle = df_google.merge(df_market, how='inner',on='Date')\n",
    "dataSetCombinadoTwitter = df_tweet.merge(df_market, how='inner', on='Date')\n",
    "\n",
    "dataSetCombinadoTwitter = labeleador(dataSetCombinadoTwitter)\n",
    "dataSetCombinadoGoogle = limpiaErroresG(dataSetCombinadoGoogle)\n",
    "dataSetCombinadoGoogle = labeleador(dataSetCombinadoGoogle)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "91c5ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizerT = CountVectorizer(max_df=0.9, min_df=2, max_features=1000, stop_words='english')\n",
    "bowT = bow_vectorizerT.fit_transform(dataSetCombinadoTwitter['Tweets'])\n",
    "\n",
    "test = train_test_split(bowT,dataSetCombinadoTwitter['Label'], random_state = 42, test_size=0.25)\n",
    "\n",
    "bow_vectorizerG = CountVectorizer(max_df=0.9, min_df=2, max_features=1000, stop_words='english')\n",
    "bowG = bow_vectorizerG.fit_transform(dataSetCombinadoGoogle['Desc'])\n",
    "\n",
    "#test = train_test_split(bowG,dataSetCombinadoGoogle['Label'], random_state = 42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "93c22317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\christian\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:673: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[<375x765 sparse matrix of type '<class 'numpy.int64'>'\n \twith 3366 stored elements in Compressed Sparse Row format>\n <125x765 sparse matrix of type '<class 'numpy.int64'>'\n \twith 1125 stored elements in Compressed Sparse Row format>\n 227    0\n 417    0\n 203    0\n 126    0\n 329    1\n       ..\n 106    0\n 270    0\n 348    1\n 435    0\n 102    1\n Name: Label, Length: 375, dtype: int64\n 361    0\n 73     1\n 374    1\n 155    0\n 104    1\n       ..\n 220    0\n 176    1\n 320    0\n 153    0\n 231    1\n Name: Label, Length: 125, dtype: int64].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-4fa32461bb9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredRegT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelLRT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    695\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[<375x765 sparse matrix of type '<class 'numpy.int64'>'\n \twith 3366 stored elements in Compressed Sparse Row format>\n <125x765 sparse matrix of type '<class 'numpy.int64'>'\n \twith 1125 stored elements in Compressed Sparse Row format>\n 227    0\n 417    0\n 203    0\n 126    0\n 329    1\n       ..\n 106    0\n 270    0\n 348    1\n 435    0\n 102    1\n Name: Label, Length: 375, dtype: int64\n 361    0\n 73     1\n 374    1\n 155    0\n 104    1\n       ..\n 220    0\n 176    1\n 320    0\n 153    0\n 231    1\n Name: Label, Length: 125, dtype: int64].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "predRegT = modelLRT.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd493c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
